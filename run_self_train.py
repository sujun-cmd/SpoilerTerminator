import os
import torch
import torch.nn as nn
import numpy as np
import joblib
import evaluate
from datasets import load_dataset, Dataset, DatasetDict
from nltk.tokenize import sent_tokenize
import nltk
from sentence_transformers import SentenceTransformer
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification, 
    TrainingArguments, 
    Trainer, 
    DataCollatorWithPadding
)

# --- 0. ç¯å¢ƒä¸é…ç½® ---
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
    nltk.download('punkt_tab')

current_dir = os.path.dirname(os.path.abspath(__file__))
# ä¹‹å‰ä¿å­˜ LR æ¨¡å‹çš„ç›®å½•
vector_model_dir = os.path.join(current_dir, "vector_models")
# æœ€ç»ˆæ¨¡å‹ä¿å­˜ç›®å½•
final_output_dir = os.path.join(current_dir, "results_final_roberta")
final_save_path = os.path.join(current_dir, "spoiler_roberta_final")

# --- 1. å¬å”¤â€œè€å¸ˆâ€ (åŠ è½½ LR å’Œ å¥å‘é‡æ¨¡å‹) ---
print("ğŸ“ æ­£åœ¨åŠ è½½ Teacher æ¨¡å‹ (LR + SentenceBERT)...")
clf_path = os.path.join(vector_model_dir, "spoiler_direction_clf.pkl")

if not os.path.exists(clf_path):
    raise FileNotFoundError("æ‰¾ä¸åˆ° LR æ¨¡å‹ï¼è¯·å…ˆè¿è¡Œä¹‹å‰çš„ vector è®­ç»ƒè„šæœ¬ã€‚")

teacher_clf = joblib.load(clf_path)

embed_model = SentenceTransformer('all-mpnet-base-v2')
if torch.cuda.is_available():
    embed_model = embed_model.to('cuda')

# --- 2. å‡†å¤‡åŸå§‹æ•°æ® ---
print("ğŸ“¦ æ­£åœ¨åŠ è½½åŸå§‹æ•°æ®é›†...")
raw_dataset = load_dataset("bhavyagiri/imdb-spoiler")

# åˆ‡åˆ†å‡ºéªŒè¯é›† (è¿™éƒ¨åˆ†ä¸åŠ¨ï¼Œç”¨æ¥è€ƒè¯•)
split = raw_dataset["train"].train_test_split(test_size=0.1, seed=42)
raw_train = split["train"]
raw_val = split["test"] # éªŒè¯é›†

# --- 3. è€å¸ˆæ¸…æ´—æ•°æ® (The Purge) ---
print("ï¿½ï¿½ è€å¸ˆæ­£åœ¨æ‰¹æ”¹ä½œä¸š (æ•°æ®æ¸…æ´—ä¸­)...")

# 3.1 æå–æ‰€æœ‰å¥å­
all_sentences = []
# ä¸ºäº†è¿½è¸ªè¿›åº¦ï¼Œæˆ‘ä»¬å…ˆä¸åˆ† batchï¼Œç›´æ¥æŠŠæ‰€æœ‰å¥å­æ‹¿å‡ºæ¥
# è¿™æ˜¯ä¸€ä¸ªå†…å­˜å¯†é›†å‹æ“ä½œï¼Œä½† V100 èŠ‚ç‚¹å†…å­˜é€šå¸¸å¤Ÿç”¨
print("æ­£åœ¨åˆ‡åˆ†å¥å­...")
for text in raw_train["text"]:
    sents = sent_tokenize(text)
    for sent in sents:
        if len(sent) < 8: continue # å¤ªçŸ­çš„ä¸è¦
        all_sentences.append(sent)

print(f"åŸå§‹å¥å­æ€»æ•°: {len(all_sentences)}")

# 3.2 è®¡ç®—å‘é‡ (Batch å¤„ç†)
print("æ­£åœ¨è®¡ç®—å‘é‡ (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")
# encode è‡ªåŠ¨å¤„ç† batching
embeddings = embed_model.encode(all_sentences, batch_size=128, show_progress_bar=True)

# 3.3 è€å¸ˆæ‰“åˆ†
print("è€å¸ˆæ­£åœ¨æ‰“åˆ†...")
probs = teacher_clf.predict_proba(embeddings)[:, 1] # è·å–â€œå‰§é€æ¦‚ç‡â€

# 3.4 ä¸¥æ ¼ç­›é€‰ (é˜ˆå€¼è¿‡æ»¤)
# ç­–ç•¥ï¼šåªä¿ç•™æå…¶ç¡®å®šçš„æ ·æœ¬
HIGH_CONFIDENCE_SPOILER = 0.7  # é«˜äºè¿™ä¸ªæ‰æ˜¯å‰§é€
HIGH_CONFIDENCE_SAFE = 0.40     # ä½äºè¿™ä¸ªæ‰æ˜¯å®‰å…¨
# ä¸­é—´çš„ (0.4 ~ 0.7) å…¨éƒ¨ä¸¢å¼ƒï¼

clean_sentences = []
clean_labels = []

print(f"æ­£åœ¨åº”ç”¨é˜ˆå€¼è¿‡æ»¤ (Safe < {HIGH_CONFIDENCE_SAFE}, Spoiler > {HIGH_CONFIDENCE_SPOILER})...")

for sent, score in zip(all_sentences, probs):
    if score > HIGH_CONFIDENCE_SPOILER:
        clean_sentences.append(sent)
        clean_labels.append(1)
    elif score < HIGH_CONFIDENCE_SAFE:
        clean_sentences.append(sent)
        clean_labels.append(0)
    # else: ä¸¢å¼ƒæ¨¡æ£±ä¸¤å¯çš„

# --- 4. æ„å»ºçº¯å‡€æ•°æ®é›† ---
clean_pos = sum(clean_labels)
clean_neg = len(clean_labels) - clean_pos

print("="*40)
print("âœ¨ æ¸…æ´—å®Œæˆï¼æ•°æ®ç»Ÿè®¡ âœ¨")
print(f"ä¿ç•™æ€»æ•°: {len(clean_labels)} (ä¸¢å¼ƒäº† {len(all_sentences) - len(clean_labels)} å¥åºŸè¯)")
print(f"çº¯å‡€å‰§é€å¥ (Label 1): {clean_pos}")
print(f"çº¯å‡€å®‰å…¨å¥ (Label 0): {clean_neg}")
ratio = clean_neg / clean_pos if clean_pos > 0 else 1.0
print(f"æ­£è´Ÿæ¯”ä¾‹: 1 : {ratio:.2f}")
print("="*40)

# è½¬å› HuggingFace Dataset
train_dataset = Dataset.from_dict({"sentence": clean_sentences, "label": clean_labels})

# å¤„ç†éªŒè¯é›† (éªŒè¯é›†æˆ‘ä»¬ä¸åšæ¸…æ´—ï¼Œä¿æŒåŸæ ·ï¼Œæˆ–è€…ç®€å•åˆ‡åˆ†ï¼Œä¸ºäº†å…¬å¹³è¯„ä¼°)
# è¿™é‡Œä¸ºäº†ä»£ç ç®€å•ï¼Œæˆ‘ä»¬å¯¹éªŒè¯é›†åªåšç®€å•åˆ‡åˆ†ï¼Œä½¿ç”¨åŸå§‹ Weak Label ä½œä¸ºå‚è€ƒ
# (è™½ç„¶éªŒè¯é›†ä¹Ÿæœ‰å™ªéŸ³ï¼Œä½†å®ƒæ˜¯æˆ‘ä»¬å”¯ä¸€çš„å‚è€ƒæ ‡å‡†)
val_sentences = []
val_labels = []
for text, label in zip(raw_val["text"], raw_val["label"]):
    for sent in sent_tokenize(text):
        if len(sent) < 8: continue
        val_sentences.append(sent)
        val_labels.append(label)
val_dataset = Dataset.from_dict({"sentence": val_sentences, "label": val_labels})

# --- 5. è®­ç»ƒå­¦ç”Ÿ (RoBERTa) ---
print("ğŸ‘¨â€ğŸ“ å­¦ç”Ÿ (RoBERTa) å¼€å§‹å­¦ä¹ ...")
model_name = "roberta-large"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

def preprocess(batch):
    return tokenizer(batch["sentence"], truncation=True, max_length=128)

tokenized_train = train_dataset.map(preprocess, batched=True)
tokenized_val = val_dataset.map(preprocess, batched=True)

# æƒé‡
class_weights = torch.tensor([1.0, ratio]).float()

class WeightedTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        labels = inputs.get("labels")
        outputs = model(**inputs)
        logits = outputs.get("logits")
        weights = class_weights.to(model.device)
        loss_fct = nn.CrossEntropyLoss(weight=weights)
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
        return (loss, outputs) if return_outputs else loss

# æŒ‡æ ‡
f1_metric = evaluate.load("f1")
precision_metric = evaluate.load("precision")
recall_metric = evaluate.load("recall")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    f1 = f1_metric.compute(predictions=preds, references=labels)["f1"]
    p = precision_metric.compute(predictions=preds, references=labels)["precision"]
    r = recall_metric.compute(predictions=preds, references=labels)["recall"]
    return {"f1": f1, "precision": p, "recall": r}

training_args = TrainingArguments(
    output_dir=final_output_dir,
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_steps=100,
    per_device_train_batch_size=32, # V100 æ€§èƒ½å…¨å¼€
    per_device_eval_batch_size=32,
    gradient_accumulation_steps=1,
    num_train_epochs=3,
    learning_rate=2e-5,
    weight_decay=0.01,
    fp16=True, # V100 å¿…å¤‡
    bf16=False,
    push_to_hub=False
)

trainer = WeightedTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    tokenizer=tokenizer,
    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),
    compute_metrics=compute_metrics
)

trainer.train()

print("ğŸ“ æœ€ç»ˆè€ƒè¯• (è¯„ä¼°)...")
metrics = trainer.evaluate()
print(f"Final F1: {metrics['eval_f1']:.4f}")
print(f"Final Precision: {metrics['eval_precision']:.4f}")
print(f"Final Recall: {metrics['eval_recall']:.4f}")

trainer.save_model(final_save_path)
print("ğŸ‰ æ¯•ä¸šäº†ï¼Self-Training æµç¨‹ç»“æŸã€‚")
