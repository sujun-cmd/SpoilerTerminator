import nltk
import os
import torch
import torch.nn as nn
import numpy as np
import evaluate
from nltk.tokenize import sent_tokenize
from datasets import load_dataset, DatasetDict
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification, 
    TrainingArguments, 
    Trainer, 
    DataCollatorWithPadding,
    pipeline
)

# --- 0. ç¯å¢ƒè®¾ç½® ---
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab')

if torch.cuda.is_available():
    torch.cuda.empty_cache()

current_dir = os.path.dirname(os.path.abspath(__file__))
output_dir_path = os.path.join(current_dir, "results_hybrid")
model_save_path = os.path.join(current_dir, "spoiler_roberta_hybrid")

# --- 1. å…³é”®è¯ (ä¿åº•ç­–ç•¥) ---
spoiler_keywords = [
    "plot twist", "twist reveal", "big reveal", "the truth is", "it turns out", "actually",
    "in reality", "hidden agenda", "secret identity", "real identity", "double identity",
    "backstory revealed", "the real reason", "foreshadowing pays off", "major turning point",
    "unexpected shift", "game changer", "crucial clue", "final clue", "the moment everything changes",
    "true intention", "the real plan", "flashback explains", "surprise appearance", "cameo reveal",
    "dies", "doesn't make it", "survives", "comes back to life", "revived",
    "was alive the whole time", "not dead", "betrays", "betrayal", "turns evil",
    "goes dark", "redemption arc", "secret sibling", "long-lost sibling", "hidden family",
    "adoption reveal", "fake death", "sacrifice", "identity swap", "body double",
    "double agent", "undercover reveal", "ending explained", "final scene", "post-credits scene",
    "after-credits reveal", "true ending", "hidden ending", "alternate ending", "final twist",
    "cliffhanger ending", "resolution", "full explanation", "gets together with", "ends up with",
    "confession scene", "love triangle resolved", "breakup", "proposal", "wedding reveal",
    "secret crush reveal", "the killer is", "the murderer is", "the culprit", "the mastermind",
    "inside job", "was planned all along", "false alibi", "unreliable narrator", "hallucination",
    "imaginary character", "not real", "dream sequence", "it was all a dream", "experiment failure",
    "switcheroo", "identity twist", "secret recording", "prophecy fulfilled", "chosen one reveal",
    "time loop revealed", "parallel universe twist", "alternate timeline", "memory wipe", "mind control",
    "the artifact's power", "true nature of the world", "simulation reveal", "the letter says",
    "the box contains", "the map shows", "the key unlocks", "hidden message", "coded message",
    "ancient secret", "the experiment worked", "the experiment failed", "major spoilers",
    "spoiler alert", "big spoiler ahead", "full plot summary", "ending breakdown",
    "all secrets explained", "hereâ€™s what really happened", "you wonâ€™t believe this part","dead","died","defeat"
]

# --- 2. AI è€å¸ˆ (è¡¥æ¼ç­–ç•¥) ---
# ä½¿ç”¨ pipeline ç®€åŒ–æ¨ç†ï¼Œé˜²æ­¢ index æé”™
print("ğŸš€ æ­£åœ¨åŠ è½½ AI è¾…åŠ©æ¨¡å‹...")
device = 0 if torch.cuda.is_available() else -1
classifier = pipeline("zero-shot-classification", model="roberta-large-mnli", device=device)

def ai_check_batch(sentences):
    """
    ä½¿ç”¨ Zero-Shot åˆ†ç±»æ¥åˆ¤æ–­æ˜¯å¦æ˜¯å‰§é€
    """
    if not sentences:
        return []
    
    candidate_labels = ["spoiler", "safe"]
    hypothesis_template = "This sentence contains a {}."
    
    # æ‰¹é‡æ¨ç†
    results = classifier(sentences, candidate_labels, hypothesis_template=hypothesis_template)
    
    # è§£æç»“æœ: å¦‚æœ label[0] æ˜¯ spoilerï¼Œåˆ™ä¸º 1
    labels = [1 if res['labels'][0] == 'spoiler' else 0 for res in results]
    return labels

# --- 3. åŠ è½½æ•°æ® ---
raw_dataset = load_dataset("bhavyagiri/imdb-spoiler")
# ä»…ä¾›æµ‹è¯•ï¼Œå¦‚æœæƒ³è·‘å…¨é‡è¯·æ³¨é‡Šä¸‹é¢è¿™è¡Œ
# raw_dataset["train"] = raw_dataset["train"].select(range(5000))

split_1 = raw_dataset["train"].train_test_split(test_size=0.1, seed=42)
dataset = DatasetDict({
    "train": split_1["train"],
    "validation": split_1["test"]
})

# --- 4. æ··åˆæ•°æ®æ¸…æ´—é€»è¾‘ ---
def process_data_hybrid(batch):
    final_sentences = []
    final_labels = []
    
    # æš‚å­˜éœ€è¦ AI åˆ¤æ–­çš„å¥å­
    ai_candidates = []
    ai_indices = [] # è®°å½•å®ƒä»¬åœ¨ final_sentences é‡Œçš„ä½ç½®ï¼Œä»¥ä¾¿å›å¡«
    
    for text, doc_label in zip(batch["text"], batch["label"]):
        sents = sent_tokenize(text)
        
        for sent in sents:
            if len(sent) < 5: continue
            
            # 1. æ­£å¸¸è¯„è®º -> å…¨æ˜¯ 0
            if doc_label == 0:
                final_sentences.append(sent)
                final_labels.append(0)
                continue
            
            # 2. å‰§é€è¯„è®º -> æ··åˆæ£€æŸ¥
            sent_lower = sent.lower()
            
            # ç­–ç•¥ A: å…³é”®è¯å‘½ä¸­ -> è‚¯å®šæ˜¯ 1 (High Precision)
            keyword_hit = False
            for kw in spoiler_keywords:
                if kw in sent_lower:
                    keyword_hit = True
                    break
            
            if keyword_hit:
                final_sentences.append(sent)
                final_labels.append(1)
            else:
                # ç­–ç•¥ B: æ²¡å‘½ä¸­å…³é”®è¯ -> æ”¾å…¥å¾…å®šåŒºï¼Œç­‰ AI åˆ¤
                # å…ˆå ä¸ªä½ï¼Œå¡« -1
                final_sentences.append(sent)
                final_labels.append(-1)
                ai_candidates.append(sent)
                ai_indices.append(len(final_labels) - 1)
    
    # 3. æ‰¹é‡ AI åˆ¤å†³
    if ai_candidates:
        # è¿™é‡Œä¸ºäº†é˜²æ­¢ OOMï¼Œå¯ä»¥å†åˆ†ä¸ªå° batchï¼Œæˆ–è€…ç›´æ¥äº¤ç»™ pipeline å¤„ç†
        # pipeline é»˜è®¤å¤„ç†åˆ—è¡¨å¾ˆç¨³
        ai_results = ai_check_batch(ai_candidates)
        
        # å›å¡«ç»“æœ
        for idx, label in zip(ai_indices, ai_results):
            final_labels[idx] = label
            
    # 4. è¿‡æ»¤æ‰ AI è®¤ä¸ºæ˜¯ 0 çš„å¥å­ (åœ¨å‰§é€è¯„è®ºé‡Œï¼Œå¦‚æœ AI å’Œå…³é”®è¯éƒ½è¯´æ˜¯ 0ï¼Œé‚£å°±ä¸¢å¼ƒï¼Œé˜²æ­¢å™ªéŸ³)
    #    ä½†ä¸ºäº†é˜²æ­¢ F1=0ï¼Œæˆ‘ä»¬æš‚æ—¶ä¿ç•™å®ƒä»¬ä½œä¸º 0ï¼Œæˆ–è€…ä¸¢å¼ƒ
    #    è¿™é‡Œé€‰æ‹©ï¼šä¸¢å¼ƒ (Label 0)ï¼Œåªä¿ç•™ æ­£å¸¸è¯„è®ºé‡Œçš„0 å’Œ å‰§é€è¯„è®ºé‡Œçš„1
    
    filtered_sentences = []
    filtered_labels = []
    
    for s, l in zip(final_sentences, final_labels):
        if l == 1:
            filtered_sentences.append(s)
            filtered_labels.append(1)
        elif l == 0:
            # è¿™é‡Œçš„ 0 å¤§éƒ¨åˆ†æ¥è‡ª doc_label=0ï¼Œå°‘é‡æ¥è‡ª AI åˆ¤ä¸º safe
            filtered_sentences.append(s)
            filtered_labels.append(0)
        # l == -1 çš„æƒ…å†µå·²ç»è¢«å¡«äº†ï¼Œå¦‚æœ AI åˆ¤ä¸º 0 (safe) ä¸”æ¥æºæ˜¯ doc_label=1
        # è¿™ç§å¥å­æ˜¯ "å‰§é€è¯„è®ºé‡Œçš„åºŸè¯"ï¼Œæˆ‘ä»¬ä¸¢å¼ƒå®ƒï¼
            
    return {"sentence": filtered_sentences, "label": filtered_labels}

print("æ­£åœ¨æ‰§è¡Œæ··åˆç­–ç•¥æ¸…æ´— (Keywords + AI)...")
processed_dataset = dataset.map(
    process_data_hybrid, 
    batched=True, 
    batch_size=50, # è°ƒå°ä¸€ç‚¹ï¼Œå› ä¸ºé‡Œé¢æœ‰æ¨¡å‹æ¨ç†
    remove_columns=dataset["train"].column_names
)

# === å…³é”®æ£€æŸ¥ç‚¹ ===
train_labels = processed_dataset["train"]["label"]
pos_count = sum(train_labels)
neg_count = len(train_labels) - pos_count

print("="*40)
print(f"æœ€ç»ˆæ•°æ®é›†ç»Ÿè®¡:")
print(f"æ€»å¥æ•°: {len(train_labels)}")
print(f"å‰§é€å¥ (Label 1): {pos_count}")
print(f"æ­£å¸¸å¥ (Label 0): {neg_count}")
print("="*40)

if pos_count < 100:
    print("ğŸš¨ ä¸¥é‡è­¦å‘Š: æ­£æ ·æœ¬å¤ªå°‘ï¼æ¨¡å‹æ ¹æœ¬å­¦ä¸åˆ°ä¸œè¥¿ã€‚")
    print("å¯èƒ½åŸå› : å…³é”®è¯æ²¡åŒ¹é…ä¸Šï¼Œä¸” AI æ¨¡å‹ä¹Ÿæ²¡è¯†åˆ«å‡ºæ¥ã€‚")
    # å¼ºåˆ¶è®¾ç½®ä¸€ä¸ªæƒé‡ï¼Œè™½ç„¶å¯èƒ½æ²¡ç”¨
    ratio = 100.0
else:
    ratio = neg_count / pos_count
    print(f"è®¡ç®—å‡ºçš„æ­£æ ·æœ¬æƒé‡: {ratio:.2f}")

class_weights = torch.tensor([1.0, ratio]).float()

# --- 5. æ­£å¼è®­ç»ƒ ---
model_name = "roberta-large" # ç”¨ Base å§ï¼ŒV100 è·‘ Large æœ‰ç‚¹æ…¢ï¼Œå…ˆè·‘é€š Base æ‹¿åˆ†
tokenizer = AutoTokenizer.from_pretrained(model_name)
def preprocess(batch):
    return tokenizer(batch["sentence"], truncation=True, max_length=128)

tokenized = processed_dataset.map(preprocess, batched=True)

class WeightedTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        labels = inputs.get("labels")
        outputs = model(**inputs)
        logits = outputs.get("logits")
        weights = class_weights.to(model.device)
        loss_fct = nn.CrossEntropyLoss(weight=weights)
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
        return (loss, outputs) if return_outputs else loss

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
# åŠ è½½å¤šä¸ªæŒ‡æ ‡
f1_metric = evaluate.load("f1")
precision_metric = evaluate.load("precision")
recall_metric = evaluate.load("recall")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    
    f1 = f1_metric.compute(predictions=preds, references=labels)["f1"]
    p = precision_metric.compute(predictions=preds, references=labels)["precision"]
    r = recall_metric.compute(predictions=preds, references=labels)["recall"]
    
    return {"f1": f1, "precision": p, "recall": r}

model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

training_args = TrainingArguments(
    output_dir=output_dir_path,
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_steps=50,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    gradient_accumulation_steps=2,
    num_train_epochs=3,
    learning_rate=2e-5,
    weight_decay=0.01,
    fp16=True, # V100 å¼€å¯
    bf16=False,
    push_to_hub=False
)

trainer = WeightedTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized["train"],
    eval_dataset=tokenized["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

print("å¼€å§‹è®­ç»ƒ...")
trainer.train()

print("è¯„ä¼°ä¸­...")
metrics = trainer.evaluate()
print(f"Final F1: {metrics['eval_f1']:.4f}")
print(f"Final Recall: {metrics['eval_recall']:.4f}")

trainer.save_model(model_save_path)
