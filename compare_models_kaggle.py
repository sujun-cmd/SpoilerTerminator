import os
import time
import json
import torch
import joblib
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, accuracy_score

# ==========================================
# 0. ç¯å¢ƒä¸è·¯å¾„è®¾ç½®
# ==========================================
device = 0 if torch.cuda.is_available() else -1
current_dir = os.path.dirname(os.path.abspath(__file__))

# æ¨¡å‹è·¯å¾„
lr_model_path = os.path.join(current_dir, "vector_models", "spoiler_direction_clf.pkl")
roberta_model_path = os.path.join(current_dir, "spoiler_roberta_final")

# æ•°æ®é›†è·¯å¾„ (æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨çš„æ˜¯è§£å‹åçš„ json)
DATA_FILE="/home/sujun/datasets/goodreads/2/goodreads_reviews_spoiler.json"
print(f"ğŸš€ è¿è¡Œè®¾å¤‡: {'GPU (' + torch.cuda.get_device_name(0) + ')' if torch.cuda.is_available() else 'CPU'}")

# ==========================================
# 1. åŠ è½½ä¸¤ä¸ªæ¨¡å‹ (é€‰æ‰‹å…¥åœº)
# ==========================================
print("\nğŸ¥Š æ­£åœ¨åŠ è½½æ¨¡å‹...")

# --- é€‰æ‰‹ A: LR Teacher (åŸºäºå‘é‡) ---
print(">>> Load: LR Teacher (MPNet + Logistic Regression)...")
try:
    if not os.path.exists(lr_model_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° LR æ¨¡å‹æ–‡ä»¶: {lr_model_path}")
    
    lr_clf = joblib.load(lr_model_path)
    
    # åŠ è½½ MPNet (ç”¨äºè®¡ç®—å¥å‘é‡)
    embed_model = SentenceTransformer('all-mpnet-base-v2')
    if torch.cuda.is_available():
        embed_model = embed_model.to('cuda')
        
except Exception as e:
    print(f"âŒ LR æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    exit()

# --- é€‰æ‰‹ B: RoBERTa Student (åŸºäºå¾®è°ƒ) ---
print(">>> Load: RoBERTa Student (Fine-tuned)...")
try:
    if not os.path.exists(roberta_model_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° RoBERTa æ¨¡å‹æ–‡ä»¶å¤¹: {roberta_model_path}")

    tokenizer = AutoTokenizer.from_pretrained(roberta_model_path)
    model = AutoModelForSequenceClassification.from_pretrained(roberta_model_path)
    
    # ä½¿ç”¨ Pipeline è¿›è¡Œæ¨ç†
    roberta_pipe = pipeline(
        "text-classification", 
        model=model, 
        tokenizer=tokenizer, 
        device=device, 
        truncation=True, 
        max_length=128,
        batch_size=64
    )
except Exception as e:
    print(f"âŒ RoBERTa æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    exit()

# ==========================================
# 2. åŠ è½½ Kaggle Goodreads æ•°æ®
# ==========================================
print(f"\nğŸ“š æ­£åœ¨è¯»å–æµ‹è¯•æ•°æ®: {DATA_FILE}")

if not os.path.exists(DATA_FILE):
    print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {DATA_FILE}")
    print("è¯·å…ˆå°† 'goodreads_reviews_spoiler.json' ä¸Šä¼ åˆ°å½“å‰ç›®å½•ï¼")
    exit()

test_sentences = []
test_labels = []

# è®¾å®šæµ‹è¯•æ ·æœ¬æ•°é‡ (å¤ªå¤§è·‘å¾—æ…¢ï¼Œ5000-10000 è¶³å¤Ÿå‡ºç»“æœ)
MAX_SAMPLES = 10000 
count = 0

try:
    # ä½¿ç”¨æ ‡å‡† open è¯»å– json æ–‡ä»¶ (JSON Lines æ ¼å¼)
    with open(DATA_FILE, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                # è§£ææ¯ä¸€è¡Œ JSON
                review = json.loads(line)
                
                # æå– review_sentences å­—æ®µ
                # æ ¼å¼: [[label, text], [label, text], ...]
                # label: 0 (safe), 1 (spoiler)
                sentences_data = review.get('review_sentences', [])
                
                for label, text in sentences_data:
                    # è¿‡æ»¤æ‰å¤ªçŸ­çš„å¥å­ (å™ªéŸ³)
                    if len(text) < 10: continue
                    
                    test_sentences.append(text)
                    test_labels.append(int(label))
                    count += 1
                    
            except Exception as e:
                continue
            
            # è¾¾åˆ°æ•°é‡é™åˆ¶åˆ™åœæ­¢
            if count >= MAX_SAMPLES:
                break
                
except Exception as e:
    print(f"è¯»å–æ•°æ®æ—¶å‡ºé”™: {e}")
    exit()

print(f"âœ… æµ‹è¯•é›†æ„å»ºå®Œæˆ: {len(test_sentences)} å¥")
print(f"   å‰§é€å¥ (Label 1): {sum(test_labels)}")
print(f"   æ­£å¸¸å¥ (Label 0): {len(test_labels) - sum(test_labels)}")
spoiler_ratio = sum(test_labels) / len(test_labels)
print(f"   å‰§é€å æ¯”: {spoiler_ratio:.2%}")

if sum(test_labels) == 0:
    print("âš ï¸ è­¦å‘Š: æµ‹è¯•é›†é‡Œæ²¡æœ‰å‰§é€å¥ï¼Œè¯„ä¼°ç»“æœå¯èƒ½æ— æ•ˆã€‚")

# ==========================================
# 3. æ¯”èµ›å¼€å§‹ (Inference)
# ==========================================

# --- Round 1: LR æ¨¡å‹ ---
print("\nğŸ”¥ Round 1: LR (Vector) Model æ¨ç†ä¸­...")
start_time = time.time()

# 1. è®¡ç®—å‘é‡ (Batch size 128)
vecs = embed_model.encode(test_sentences, batch_size=128, show_progress_bar=True)
# 2. é¢„æµ‹
lr_preds = lr_clf.predict(vecs)

lr_time = time.time() - start_time
print(f"LR è€—æ—¶: {lr_time:.2f} ç§’")

# --- Round 2: RoBERTa æ¨¡å‹ ---
print("\nğŸ”¥ Round 2: RoBERTa (Student) Model æ¨ç†ä¸­...")
start_time = time.time()

# Pipeline æ¨ç†
roberta_results = roberta_pipe(test_sentences)
# æå–æ ‡ç­¾ (LABEL_1 -> 1, LABEL_0 -> 0)
roberta_preds = [1 if res['label'] == 'LABEL_1' else 0 for res in roberta_results]

roberta_time = time.time() - start_time
print(f"RoBERTa è€—æ—¶: {roberta_time:.2f} ç§’")

# ==========================================
# 4. ç»“æœå¯¹æ¯”ä¸ç»“ç®—
# ==========================================

def print_metrics(name, y_true, y_pred, time_taken):
    print(f"\nğŸ“Š {name} æˆç»©å•")
    print("-" * 40)
    print(f"æ¨ç†é€Ÿåº¦ (QPS): {len(y_true)/time_taken:.2f} sentences/sec")
    
    p = precision_score(y_true, y_pred, zero_division=0)
    r = recall_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)
    acc = accuracy_score(y_true, y_pred)
    
    print(f"Accuracy:  {acc:.4f}")
    print(f"Precision: {p:.4f}")
    print(f"Recall:    {r:.4f}")
    print(f"F1 Score:  {f1:.4f}")

print("\n" + "="*60)
print("ğŸ† æœ€ç»ˆå¯¹å†³ç»“æœ (Movie Models -> Book Reviews)")
print("="*60)

print_metrics("LR Teacher (MPNet Vectors)", test_labels, lr_preds, lr_time)
print_metrics("RoBERTa Student (Fine-tuned)", test_labels, roberta_preds, roberta_time)

print("="*60)

# ==========================================
# 5. å·®å¼‚åˆ†æ (Disagreement Analysis)
# ==========================================
print("\nğŸ” å·®å¼‚æ ·æœ¬åˆ†æ (çœ‹è°æ›´å‡†ï¼Ÿ)")
print("åªæ˜¾ç¤ºï¼šçœŸå®æ ‡ç­¾æ˜¯å‰§é€ï¼Œä½†æ¨¡å‹äº§ç”Ÿäº†åˆ†æ­§çš„ä¾‹å­")
print("-" * 60)

count = 0
for i in range(len(test_sentences)):
    if count >= 10: break
    
    # æˆ‘ä»¬åªå…³å¿ƒçœŸå®çš„å‰§é€å¥å­ (Label 1)
    if test_labels[i] == 1:
        # å¦‚æœä¸¤æ¨¡å‹é¢„æµ‹ç»“æœä¸åŒ
        if lr_preds[i] != roberta_preds[i]:
            lr_res = "âœ…" if lr_preds[i]==1 else "âŒ"
            rob_res = "âœ…" if roberta_preds[i]==1 else "âŒ"
            
            print(f"å¥å­: {test_sentences[i][:150]}...") # æˆªæ–­æ˜¾ç¤º
            print(f"LR é¢„æµ‹: {lr_res} ({lr_preds[i]}) | RoBERTa é¢„æµ‹: {rob_res} ({roberta_preds[i]})")
            print("-" * 30)
            count += 1
