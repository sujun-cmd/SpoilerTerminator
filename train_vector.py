import os
import torch
import numpy as np
import joblib
from datasets import load_dataset
from nltk.tokenize import sent_tokenize
import nltk
from sentence_transformers import SentenceTransformer
from sklearn.linear_model import LogisticRegression

# --- ç¯å¢ƒè®¾ç½® ---
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
    nltk.download('punkt_tab')

current_dir = os.path.dirname(os.path.abspath(__file__))
output_dir = os.path.join(current_dir, "vector_models")
os.makedirs(output_dir, exist_ok=True)

# --- 1. åŠ è½½ä¸åˆ‡åˆ†æ•°æ® (å…³é”®ä¿®æ”¹) ---
print("æ­£åœ¨åŠ è½½æ•°æ®é›†...")
raw_dataset = load_dataset("bhavyagiri/imdb-spoiler")

# [å…³é”®ä¿®æ”¹] æ‰‹åŠ¨åˆ‡åˆ†ï¼Œä¿ç•™ 20% ä½œä¸ºæµ‹è¯•é›†ï¼ˆä¸å‚ä¸è®­ç»ƒï¼ï¼‰
# seed=42 ä¿è¯æ¯æ¬¡åˆ‡åˆ†çš„ç»“æœéƒ½ä¸€æ ·
split = raw_dataset["train"].train_test_split(test_size=0.2, seed=42)
train_data = split["train"]  # åªæ‹¿ 80% ç”¨æ¥è®­ç»ƒ

print("æ­£åœ¨åˆ‡åˆ†å¥å­å¹¶æ„å»ºè®­ç»ƒæ•°æ®...")
train_sentences = []
train_labels = [] 

for text, doc_label in zip(train_data["text"], train_data["label"]):
    sents = sent_tokenize(text)
    for sent in sents:
        if len(sent) < 10: continue 
        train_sentences.append(sent)
        train_labels.append(doc_label)

print(f"è®­ç»ƒé›†å¥å­æ€»æ•°: {len(train_sentences)}")
print(f"å…¶ä¸­æ­£æ ·æœ¬ (Label 1): {sum(train_labels)}")

# --- 2. è®¡ç®—å¥å‘é‡ ---
print("ğŸš€ æ­£åœ¨åŠ è½½å¥å‘é‡æ¨¡å‹ (all-mpnet-base-v2)...")
embed_model = SentenceTransformer('all-mpnet-base-v2')
if torch.cuda.is_available():
    embed_model = embed_model.to('cuda')
    print("ä½¿ç”¨ GPU åŠ é€Ÿè®¡ç®—å‘é‡")

print("æ­£åœ¨è®¡ç®—å‘é‡ (Encoding)...")
X_train = embed_model.encode(train_sentences, batch_size=64, show_progress_bar=True)
y_train = np.array(train_labels)

# --- 3. å¯»æ‰¾â€œå‰§é€æ–¹å‘â€ ---
print("æ­£åœ¨è®¡ç®—å‰§é€ç‰¹å¾æ–¹å‘ (Logistic Regression)...")
clf = LogisticRegression(
    random_state=42, 
    solver='liblinear', 
    class_weight='balanced', 
    max_iter=1000,
    C=1.0
)
clf.fit(X_train, y_train)
print("è®­ç»ƒå®Œæˆï¼")

# --- 4. ä¿å­˜æ¨¡å‹ ---
joblib.dump(clf, os.path.join(output_dir, "spoiler_direction_clf.pkl"))
print(f"æ¨¡å‹å·²ä¿å­˜è‡³: {output_dir}")
