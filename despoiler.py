import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

class SpoilerRewriter:
    def __init__(self, model_id="Qwen/Qwen2.5-14B-Instruct"):
        """
        åˆå§‹åŒ–æ”¹å†™å™¨ï¼ŒåŠ è½½ 4-bit é‡åŒ–çš„ LLMã€‚
        """
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸš€ [Rewriter] æ­£åœ¨åŠ è½½æ”¹å†™æ¨¡å‹: {model_id} ...")
        
        try:
            # V100 ä¸“ç”¨é…ç½®: 4-bit é‡åŒ– + Float16
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16 
            )
            
            self.tokenizer = AutoTokenizer.from_pretrained(model_id)
            self.model = AutoModelForCausalLM.from_pretrained(
                model_id,
                quantization_config=bnb_config,
                device_map="auto"
            )
            print("âœ… [Rewriter] æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            
        except Exception as e:
            print(f"âŒ [Rewriter] æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise e

    def rewrite(self, text):
        """
        è¾“å…¥ä¸€æ®µå‰§é€æ–‡æœ¬ï¼Œè¿”å›æ”¹å†™åçš„ Teaserã€‚
        """
        messages = [
            {"role": "system", "content": "You are a professional movie editor. Rewrite the spoiler into a vague, suspenseful plot teaser. You must retain the initial emotional bias (positive/negative review). Do NOT reveal names of characters who die, the killer's identity, or the specific ending. Keep it concise."},
            {"role": "user", "content": f"Rewrite this spoiler: '{text}'"}
        ]
        
        # æ„å»º Chat æ ¼å¼ Prompt
        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = self.tokenizer([prompt], return_tensors="pt").to(self.model.device)
        
        # è·å– input é•¿åº¦ä»¥ä¾¿åˆ‡ç‰‡ï¼Œåªè¿”å›ç”Ÿæˆçš„ç­”æ¡ˆ
        input_len = inputs.input_ids.shape[1]
        
        with torch.no_grad():
            generated_ids = self.model.generate(
                **inputs,
                max_new_tokens=64, # æ”¹å†™ä¸éœ€è¦å¤ªé•¿
                temperature=0.7,
                top_p=0.9,
                do_sample=True
            )
        
        # åˆ‡ç‰‡ï¼šå»æ‰ Promptï¼Œåªç•™å›ç­”
        output_ids = generated_ids[0][input_len:]
        response = self.tokenizer.decode(output_ids, skip_special_tokens=True)
        
        return response.strip()

# å¦‚æœç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶ï¼Œè¿›è¡Œç®€å•æµ‹è¯•
if __name__ == "__main__":
    rewriter = SpoilerRewriter()
    test_sent = "Bruce Willis is actually a ghost at the end."
    print(f"Original: {test_sent}")
    print(f"Rewritten: {rewriter.rewrite(test_sent)}")