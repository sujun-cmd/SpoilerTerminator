import os
import json
import torch
import nltk
from nltk.tokenize import sent_tokenize
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification, 
    AutoModelForCausalLM, 
    pipeline, 
    BitsAndBytesConfig
)
from tqdm import tqdm

# ==========================================
# 0. é…ç½®ä¸ç¯å¢ƒ
# ==========================================
device = 0 if torch.cuda.is_available() else -1
print(f"ğŸš€ è¿è¡Œè®¾å¤‡: {'GPU' if device==0 else 'CPU'}")

# è·¯å¾„è®¾ç½®
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
DETECTOR_PATH = os.path.join(CURRENT_DIR, "spoiler_roberta_final") # ä½ çš„æ£€æµ‹æ¨¡å‹
REWRITER_ID = "Qwen/Qwen2.5-7B-Instruct"                           # ä½ çš„æ”¹å†™æ¨¡å‹

INPUT_FILE = os.path.join(CURRENT_DIR, "reviews.txt")
OUTPUT_FILE = os.path.join(CURRENT_DIR, "despoiled_reviews.txt")

# ç¡®ä¿ NLTK æ•°æ®å­˜åœ¨
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

# ==========================================
# 1. åŠ è½½æ¨¡å‹ (Detector & Rewriter)
# ==========================================

print("\nğŸ“¦ æ­£åœ¨åŠ è½½æ¨¡å‹...")

# --- A. åŠ è½½å‰§é€æ£€æµ‹å™¨ (RoBERTa) ---
print(f"   [1/2] Loading Detector: {DETECTOR_PATH} ...")
try:
    det_tokenizer = AutoTokenizer.from_pretrained(DETECTOR_PATH)
    det_model = AutoModelForSequenceClassification.from_pretrained(DETECTOR_PATH)
    # ä½¿ç”¨ pipeline åŠ é€Ÿæ¨ç†
    detector = pipeline(
        "text-classification", 
        model=det_model, 
        tokenizer=det_tokenizer, 
        device=device, 
        truncation=True, 
        max_length=128
    )
except Exception as e:
    print(f"âŒ æ£€æµ‹æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    exit()

# --- B. åŠ è½½å‰§é€æ”¹å†™å™¨ (Qwen LLM) ---
print(f"   [2/2] Loading Rewriter: {REWRITER_ID} ...")
try:
    # 4-bit é‡åŒ–é…ç½® (ä¸ºäº†çœæ˜¾å­˜)
    bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16  # V100 å…³é”®è®¾ç½®ï¼ä¸è¦ç”¨ bfloat16
)
    
    rw_tokenizer = AutoTokenizer.from_pretrained(REWRITER_ID)
    rw_model = AutoModelForCausalLM.from_pretrained(
        REWRITER_ID,
        quantization_config=bnb_config,
        device_map="auto"
    )
except Exception as e:
    print(f"âŒ æ”¹å†™æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    exit()

# ==========================================
# 2. å®šä¹‰åŠŸèƒ½å‡½æ•°
# ==========================================

def rewrite_spoiler(text):
    """ä½¿ç”¨ LLM æ”¹å†™å‰§é€å¥å­"""
    messages = [
        {"role": "system", "content": "You are a professional movie editor. Rewrite the spoiler into a vague, suspenseful plot teaser. Do NOT reveal names of characters who die, the killer's identity, or the specific ending. Keep it concise."},
        {"role": "user", "content": f"Rewrite this spoiler: '{text}'"}
    ]
    
    prompt = rw_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = rw_tokenizer([prompt], return_tensors="pt").to(rw_model.device)
    
    # è·å– input é•¿åº¦ä»¥ä¾¿åˆ‡ç‰‡
    input_len = inputs.input_ids.shape[1]
    
    with torch.no_grad():
        generated_ids = rw_model.generate(
            **inputs,
            max_new_tokens=64, # æ”¹å†™ä¸éœ€è¦å¤ªé•¿
            temperature=0.7,
            top_p=0.9,
            do_sample=True
        )
    
    # åªæå–ç”Ÿæˆçš„å›å¤éƒ¨åˆ†
    output_ids = generated_ids[0][input_len:]
    response = rw_tokenizer.decode(output_ids, skip_special_tokens=True)
    return response.strip()

def process_single_review(review_json):
    """å¤„ç†å•æ¡è¯„è®ºï¼šåˆ†å¥ -> æ£€æµ‹ -> æ”¹å†™ -> é‡ç»„"""
    original_text = review_json.get("text", "")
    if not original_text:
        return None

    sentences = sent_tokenize(original_text)
    processed_sentences = []
    
    # æ ‡è®°è¿™ç¯‡è¯„è®ºæ˜¯å¦åŒ…å«å‰§é€
    has_spoiler = False
    
    # æ‰¹é‡æ£€æµ‹ (è™½ç„¶è¿™é‡Œæ˜¯é€å¥å¾ªç¯ï¼Œä½†å¯¹äºé•¿è¯„è®ºå¯ä»¥å…ˆæ”’ batchï¼Œè¿™é‡Œä¸ºäº†é€»è¾‘æ¸…æ™°é€å¥å¤„ç†)
    # å¯¹äºç”Ÿäº§ç¯å¢ƒï¼Œå»ºè®®å…ˆ flatten æ‰€æœ‰å¥å­åš batch inference
    
    # è·å–æ£€æµ‹ç»“æœ
    preds = detector(sentences)
    
    for sent, pred in zip(sentences, preds):
        label = pred['label'] # LABEL_0 (Safe) or LABEL_1 (Spoiler)
        score = pred['score']
        
        # è®¾å®šé˜ˆå€¼ï¼šå¦‚æœæ¨¡å‹éå¸¸æœ‰ä¿¡å¿ƒæ˜¯å‰§é€ (>0.8)ï¼Œæˆ–è€…æ˜¯ LABEL_1
        is_spoiler_sent = (label == 'LABEL_1')
        
        if is_spoiler_sent:
            has_spoiler = True
            print(f"   ğŸš¨ å‘ç°å‰§é€: {sent[:50]}...")
            # è°ƒç”¨ LLM æ”¹å†™
            safe_version = rewrite_spoiler(sent)
            print(f"      âœ¨ æ”¹å†™ä¸º: {safe_version[:50]}...")
            processed_sentences.append(safe_version)
        else:
            processed_sentences.append(sent)
            
    # é‡ç»„è¯„è®º
    final_text = " ".join(processed_sentences)
    
    return {
        "original_text": original_text,
        "processed_text": final_text,
        "is_spoiler_review": has_spoiler,
        "original_json": review_json # ä¿ç•™åŸå§‹å…ƒæ•°æ®
    }

# ==========================================
# 3. ä¸»æµç¨‹
# ==========================================

def main():
    print(f"\nğŸš€ å¼€å§‹å¤„ç†æ–‡ä»¶: {INPUT_FILE}")
    
    if not os.path.exists(INPUT_FILE):
        print("âŒ æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶ï¼è¯·å…ˆåˆ›å»º reviews.txt")
        return

    results = []
    
    # è¯»å–æ‰€æœ‰è¡Œ
    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        lines = f.readlines()
        
    print(f"ğŸ“š æ€»å…±æœ‰ {len(lines)} æ¡è¯„è®ºå¾…å¤„ç†...")
    
    # è¿›åº¦æ¡å¤„ç†
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f_out:
        for line in tqdm(lines):
            try:
                data = json.loads(line.strip())
            except json.JSONDecodeError:
                continue
                
            # å¤„ç†
            result = process_single_review(data)
            
            if result:
                # å†™å…¥ç»“æœæ–‡ä»¶ (JSON Lines æ ¼å¼)
                f_out.write(json.dumps(result, ensure_ascii=False) + "\n")
                # å¼ºåˆ¶åˆ·æ–°ç¼“å†²åŒºï¼Œé˜²æ­¢ç¨‹åºä¸­æ–­ä¸¢å¤±æ•°æ®
                f_out.flush()
                
    print(f"\nâœ… å¤„ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³: {OUTPUT_FILE}")

if __name__ == "__main__":
    main()