import os
import json
import torch
import nltk
from nltk.tokenize import sent_tokenize
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
from tqdm import tqdm

# å¼•å…¥æˆ‘ä»¬åˆšæ‰å†™çš„æ¨¡å—
from despoiler import SpoilerRewriter

# ==========================================
# 0. é…ç½®ä¸ç¯å¢ƒ
# ==========================================
device = 0 if torch.cuda.is_available() else -1
print(f"ğŸš€ ä¸»ç¨‹åºè¿è¡Œè®¾å¤‡: {'GPU' if device==0 else 'CPU'}")

# è·¯å¾„è®¾ç½®
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
# ä½ çš„æ£€æµ‹æ¨¡å‹è·¯å¾„ (RoBERTa Student)
DETECTOR_PATH = os.path.join(CURRENT_DIR, "spoiler_roberta_final") 

INPUT_FILE = os.path.join(CURRENT_DIR, "reviews.txt")
OUTPUT_FILE = os.path.join(CURRENT_DIR, "despoiled_reviews.txt")

# ç¡®ä¿ NLTK æ•°æ®å­˜åœ¨
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

# ==========================================
# 1. åŠ è½½æ¨¡å‹
# ==========================================

print("\nğŸ“¦ åˆå§‹åŒ– Pipeline...")

# --- A. åŠ è½½å‰§é€æ£€æµ‹å™¨ (RoBERTa) ---
print(f"   [1/2] Loading Detector: {DETECTOR_PATH} ...")
try:
    if not os.path.exists(DETECTOR_PATH):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ£€æµ‹æ¨¡å‹: {DETECTOR_PATH}")

    det_tokenizer = AutoTokenizer.from_pretrained(DETECTOR_PATH)
    det_model = AutoModelForSequenceClassification.from_pretrained(DETECTOR_PATH)
    
    # ä½¿ç”¨ pipeline åŠ é€Ÿæ¨ç†
    detector = pipeline(
        "text-classification", 
        model=det_model, 
        tokenizer=det_tokenizer, 
        device=device, 
        truncation=True, 
        max_length=128
    )
    print("âœ… [Detector] åŠ è½½æˆåŠŸ")
except Exception as e:
    print(f"âŒ [Detector] åŠ è½½å¤±è´¥: {e}")
    exit()

# --- B. åŠ è½½å‰§é€æ”¹å†™å™¨ (ä» despoiler.py) ---
print(f"   [2/2] Loading Rewriter (LLM) ...")
try:
    # å®ä¾‹åŒ–æˆ‘ä»¬åœ¨ despoiler.py é‡Œå†™çš„ç±»
    # é»˜è®¤åŠ è½½ Qwen2.5-14B-Instruct
    rewriter = SpoilerRewriter() 
except Exception as e:
    print(f"âŒ [Rewriter] åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ˜¾å­˜æˆ– despoiler.py: {e}")
    exit()

# ==========================================
# 2. æ ¸å¿ƒå¤„ç†é€»è¾‘
# ==========================================

def process_single_review(review_json):
    """å¤„ç†å•æ¡è¯„è®ºï¼šåˆ†å¥ -> æ£€æµ‹ -> æ”¹å†™ -> é‡ç»„"""
    original_text = review_json.get("text", "")
    if not original_text:
        return None

    sentences = sent_tokenize(original_text)
    processed_sentences = []
    
    # æ ‡è®°è¿™ç¯‡è¯„è®ºæ˜¯å¦åŒ…å«å‰§é€
    has_spoiler = False
    
    # æ‰¹é‡æ£€æµ‹ (ä¼ å…¥ list)
    try:
        preds = detector(sentences)
    except Exception as e:
        print(f"âš ï¸ æ£€æµ‹å‡ºé”™: {e}, è·³è¿‡æ­¤è¯„è®º")
        return None
    
    for sent, pred in zip(sentences, preds):
        label = pred['label'] # LABEL_0 (Safe) or LABEL_1 (Spoiler)
        # score = pred['score']
        
        # åˆ¤æ–­é€»è¾‘: LABEL_1 ä¸ºå‰§é€
        is_spoiler_sent = (label == 'LABEL_1')
        
        if is_spoiler_sent:
            has_spoiler = True
            # ä¸ºäº†æ—¥å¿—å¥½çœ‹ï¼Œåªæ‰“å°å‰50ä¸ªå­—ç¬¦
            clean_sent = sent.replace('\n', ' ')
            print(f"   ğŸš¨ å‘ç°å‰§é€: {clean_sent[:50]}...")
            
            # è°ƒç”¨ LLM æ”¹å†™
            safe_version = rewriter.rewrite(sent)
            print(f"      âœ¨ æ”¹å†™ä¸º: {safe_version[:50]}...")
            
            processed_sentences.append(safe_version)
        else:
            processed_sentences.append(sent)
            
    # é‡ç»„è¯„è®º
    final_text = " ".join(processed_sentences)
    
    return {
        "original_text": original_text,
        "processed_text": final_text,
        "is_spoiler_review": has_spoiler,
        "original_json": review_json
    }

# ==========================================
# 3. ä¸»å¾ªç¯
# ==========================================

def main():
    print(f"\nğŸš€ å¼€å§‹å¤„ç†æ–‡ä»¶: {INPUT_FILE}")
    
    if not os.path.exists(INPUT_FILE):
        print("âŒ æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶ï¼è¯·å…ˆç¡®ä¿ reviews.txt å­˜åœ¨ã€‚")
        return

    # è¯»å–æ‰€æœ‰è¡Œ
    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        lines = f.readlines()
        
    print(f"ğŸ“š å¾…å¤„ç†è¯„è®ºæ•°: {len(lines)}")
    
    # è¿›åº¦æ¡å¤„ç†
    processed_count = 0
    spoiler_count = 0
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f_out:
        for line in tqdm(lines, desc="Processing"):
            try:
                line = line.strip()
                if not line: continue
                
                data = json.loads(line)
            except json.JSONDecodeError:
                continue
                
            # å¤„ç†
            result = process_single_review(data)
            
            if result:
                # å†™å…¥ç»“æœæ–‡ä»¶
                f_out.write(json.dumps(result, ensure_ascii=False) + "\n")
                f_out.flush() # å®æ—¶ä¿å­˜
                
                processed_count += 1
                if result['is_spoiler_review']:
                    spoiler_count += 1
                
    print(f"\nâœ… å…¨éƒ¨å®Œæˆï¼")
    print(f"   - æ€»å¤„ç†: {processed_count}")
    print(f"   - å«å‰§é€: {spoiler_count}")
    print(f"   - ç»“æœä¿å­˜è‡³: {OUTPUT_FILE}")

if __name__ == "__main__":
    main()